{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ctran Breadcrumb Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definitions\n",
    "\n",
    "**reading**: A **_reading_** is a unique entry from a c-tran breadcrumb data collection source (e.g. a bus pushing data). A **_reading_** is composed of multiple fields and is considered well-formed if all fields are present and all fields are of the correct datatype. Additionally, a **_reading_** is considered unique if it's well-formed and the set of it's field values are unique throughout the entirety of the data. In practice, each reading relates to each row in a database or dataframe.\n",
    "\n",
    "At the time of writing, a **_reading_** contains these fields with these datatypes:\n",
    "\n",
    "| Field name | EVENT_NO_TRIP | OPD_DATE | VEHICLE_ID | METERS  | ACT_TIME | GPS_LONGITUDE | GPS_LATITUDE |\n",
    "|------------|---------------|----------|------------|---------|----------|---------------|--------------|\n",
    "| Datatype   | integer       | date     | integer    | integer | integer  | float w/ precision 6 | float w/ precision 6|\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "| Field name | EVENT_NO_TRIP | OPD_DATE | VEHICLE_ID | METERS  | ACT_TIME | GPS_LONGITUDE | GPS_LATITUDE |\n",
    "|------------|---------------|----------|------------|---------|----------|---------------|--------------|\n",
    "| Values     | 152011646     | 24-FEB-20| 1776       | 77999   | 28558    | -122.579383   | 45.533608    |\n",
    "\n",
    "**vehicle**: A **_vehicle_** is any datasource sending valid readings over time. Additionally, it must have a unique `VEHICLE_ID` which is sent within each reading. \n",
    "\n",
    "**path**: A **_path_** is a line connecting two points in 2D-space by the shortest path (i.e. as the crow flies). A point is defined as an `x`-`y` coordinate pair and its units are inconsequential -- as long as they're the same for botht the `x` and `y` coordintate. In practice, a point looks like a longitude-latitude coordinate pair, or an x-y coordinate pair with a known origin (e.g. the bottom left corner of Clark County). It's also important to note that, for the purposes of this project, the surface of any geodetic system (e.g. a sphere or the earth) is considered 2D-space. That is, the **_path_** between two points on the earth would be \"curved\" as it travels across the surface of it and not \"through\" it.\n",
    "\n",
    "- route\n",
    "- day\n",
    "- iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema Definition\n",
    "\n",
    "For the purposes of this notebook the schema is based off of pandas however, there there is a schema that defines a table within a postgresql database which is also defined here. The only notable difference is the datatype of the `GPS_LATITUDE` and `GPS_LONGITUDE` fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Grab all tsv data from the `data` directory\n",
    "ALL_DATA_FILES = glob(\"../data/**/*.csv\")\n",
    "\n",
    "def load_data():\n",
    "    ### Data columns are:\n",
    "    ### \"EVENT_NO_TRIP\"\t\"OPD_DATE\"\t\"VEHICLE_ID\"\t\"METERS\"\t\"ACT_TIME\"\t\"GPS_LONGITUDE\"\t\"GPS_LATITUDE\"\n",
    "\n",
    "    # Taken and modified from https://stackoverflow.com/a/21232849/4668680\n",
    "    return pd.concat(\n",
    "        [pd.read_csv(_file, parse_dates=[1]) for _file in ALL_DATA_FILES], ignore_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.13 s ± 85.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n1 df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df)\n",
    "UNIQUE_TRIP_NOS = df[\"EVENT_NO_TRIP\"].unique()\n",
    "UNIQUE_DATES = df[\"OPD_DATE\"].unique()\n",
    "UNIQUE_VEHICLES = df[\"VEHICLE_ID\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which dates are tracked, are these weekdays or weekend days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "\n",
    "# Test all dates are weekdays\n",
    "unique_days = pd.Series(UNIQUE_DATES)\n",
    "# print(unique_days.dt.day_name())\n",
    "\n",
    "is_all_weekdays = pd.Series([(day != \"Saturday\" and day != \"Sunday\") for day in unique_days]).all()\n",
    "print(is_all_weekdays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min   2020-02-24\n",
       "max   2020-03-20\n",
       "Name: OPD_DATE, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.OPD_DATE.agg([\"min\", \"max\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are the same vehicles tracked each day or does it change from day to day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "date_group = df.groupby(\"OPD_DATE\")\n",
    "\n",
    "df2 = date_group.apply(lambda x: len(x[\"VEHICLE_ID\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPD_DATE\n",
       "2020-02-24    100\n",
       "2020-02-25     95\n",
       "2020-02-26    100\n",
       "2020-02-27     97\n",
       "2020-02-28     99\n",
       "2020-03-02    100\n",
       "2020-03-03     97\n",
       "2020-03-04     97\n",
       "2020-03-05     99\n",
       "2020-03-06     98\n",
       "2020-03-09    103\n",
       "2020-03-10    102\n",
       "2020-03-11    100\n",
       "2020-03-12    102\n",
       "2020-03-13    102\n",
       "2020-03-16    102\n",
       "2020-03-17    104\n",
       "2020-03-18    104\n",
       "2020-03-19    101\n",
       "2020-03-20     99\n",
       "dtype: int64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ACT_TIME columns seems to represent the time at which the event occurred. what are its units?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min     4.181111\n",
       "max    25.683611\n",
       "Name: ACT_TIME, dtype: float64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's units appear to be seconds offset from midnight; this is the min/max time in hours\n",
    "df.ACT_TIME.agg([\"min\", \"max\"]).div(60 * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relating to last: Can you convert this to datetime format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         2020-03-09 05:02:53\n",
      "1         2020-03-09 05:02:58\n",
      "2         2020-03-09 05:03:03\n",
      "3         2020-03-09 05:03:08\n",
      "4         2020-03-09 05:03:13\n",
      "                  ...        \n",
      "8623465   2020-03-20 18:35:05\n",
      "8623466   2020-03-20 18:35:10\n",
      "8623467   2020-03-20 18:35:15\n",
      "8623468   2020-03-20 18:35:20\n",
      "8623469   2020-03-20 18:35:25\n",
      "Name: ACT_DTG, Length: 8623470, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# TODO: This would be more useful related to vehicle IDs\n",
    "# TODO: This doesn't appear to maintain order\n",
    "df[\"ACT_DTG\"] = df.OPD_DATE + pd.to_timedelta(df.ACT_TIME, unit=\"s\")\n",
    "print(df.ACT_DTG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are the events spaced evenly in time or are the events more frequent at specific times of day (for a given vehicle)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the delta from reading to reading; is it always 5 seconds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([   1776,    1776,    1776,    1776,    1776,    1776,    1776,\n",
      "               1776,    1776,    1776,\n",
      "            ...\n",
      "            1298380, 1298380, 1298380, 1298380, 1298380, 1298380, 1298380,\n",
      "            1298380, 1298380, 1298380],\n",
      "           dtype='int64', name='VEHICLE_ID', length=8623470)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# What is the METERS field?\n",
    "index = pd.Index(df.VEHICLE_ID)\n",
    "# Meters with vehicle id index\n",
    "meters_series = pd.Series(df.METERS, index=index)\n",
    "\n",
    "monotonic_meters = pd.Series(meters_series.groupby(level=0).is_monotonic_increasing)\n",
    "print(monotonic_meters.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which route has the largest birds-eye distance? The smallest distance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           GPS_LATITUDE            GPS_LONGITUDE            \n",
      "                    min        max           min         max\n",
      "VEHICLE_ID                                                  \n",
      "1002                NaN        NaN           NaN         NaN\n",
      "1776          45.494440  45.736335   -122.689590 -122.503760\n",
      "2215          45.494420  45.721972   -122.689602 -122.503748\n",
      "2218          45.494393  45.736378   -122.689605 -122.503738\n",
      "2220          45.494432  45.780962   -122.702240 -122.503760\n",
      "...                 ...        ...           ...         ...\n",
      "1254260       45.494432  45.722098   -122.702247 -122.503723\n",
      "1254280       45.494422  45.721990   -122.689585 -122.503735\n",
      "1254282       45.494375  45.724643   -122.689592 -122.503718\n",
      "1254300       45.494418  45.723113   -122.702272 -122.503745\n",
      "1298380       45.505045  45.780973   -122.686907 -122.475703\n",
      "\n",
      "[117 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "index = pd.MultiIndex.from_frame(df[[\"OPD_DATE\", \"VEHICLE_ID\"]])\n",
    "\n",
    "# Index by unique vehicles per day\n",
    "wanted_columns = [\"GPS_LATITUDE\", \"GPS_LONGITUDE\"]\n",
    "lat_long_df = pd.DataFrame(\n",
    "    df[wanted_columns].to_numpy(), columns=wanted_columns, index=index\n",
    ")\n",
    "\n",
    "# TODO: Why does vehicle id 1002 have NaN\n",
    "# Intermediate step to min/max lat/long of each vehicle (across all days)\n",
    "all_v_min_max = lat_long_df.groupby(\"VEHICLE_ID\").agg([\"min\", \"max\"]).reindex(axis=1)\n",
    "print(all_v_min_max)\n",
    "\n",
    "# idx = pd.IndexSlice\n",
    "# lat_mins = all_v_min_max.loc[:,[\"GPS_LATITUDE\"]]\n",
    "# maxs = all_v_min_max.loc[:,idx[:,\"max\"]]\n",
    "# print(lat_mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use datetimes instead of strings for date comparisons\n",
    "for bus_id in UNIQUE_VEHICLES:\n",
    "    assert df[\n",
    "        (df.OPD_DATE == \"20-MAR-20\") & (df.VEHICLE_ID == bus_id)\n",
    "    ].METERS.is_monotonic, f\"{bus_id} METERS is not monotonically increasing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    POLYGON ((0.00000 0.00000, 1.00000 0.00000, 1....\n",
       "1    POLYGON ((0.00000 0.00000, 1.00000 0.00000, 1....\n",
       "2    POLYGON ((2.00000 0.00000, 3.00000 0.00000, 3....\n",
       "dtype: geometry"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas\n",
    "from shapely.geometry import Polygon\n",
    "p1 = Polygon([(0, 0), (1, 0), (1, 1)])\n",
    "p2 = Polygon([(0, 0), (1, 0), (1, 1), (0, 1)])\n",
    "p3 = Polygon([(2, 0), (3, 0), (3, 1), (2, 1)])\n",
    "g = geopandas.GeoSeries([p1, p2, p3])\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
